<h1 id="how-to-learn-from-experience">How to learn from experience?</h1>
<p>In this blog post we investigate the effect of 3 different forms of experience replay on 4 different environments. As an additional experiment, we also investigate the effect of setting the hyperparameter of these methods to different values.</p>
<h2 id="deep-q-networks-and-experience-replay">Deep Q Networks and Experience Replay?</h2>
<p>Q-learning has shown its usefulness on a lot of different tasks, but how does this method scale to more complex issues, like real-world problems? The number of states and actions can grow exponentially, which makes it infeasible to store Q values in a tabular fashion. The reinforcement learning (RL) community has found a solution to this in Deep Q Networks (DQN), where Q-learning is infused with Deep Learning. This is a ‘simple’ idea where we replace the Q Learning’s table with a neural network that tries to approximate Q Values instead. One problem that we face during training DQN’s however, is that in RL, the agent learns from experiences. It uses each experience or sample transitions, e.g. (state, action, reward, new state), to update its internal beliefs. However, when we do this in an on-policy fashion, the data we sample is sequential. So when we feed it to our network, the sequential nature of the data will cause it to have a strong temporal correlation. Neural networks (NNs) were not made with this kind of data in mind. There are two main reasons for this:</p>
<ol type="1">
<li>NN’s are based on Stochastic Gradient Descent methods which are based on the assumption that the data is i.i.d., identically and independently distributed. Since each experience is based on previous experiences, this assumption does not hold.</li>
<li>DQN’s like most NN’s suffer from the problem that they tend to forget data they have seen in the past. In standard RL algorithms, this wouldn’t be a problem as experiences are discarded as soon as they are used. However, in the case of NN’s it is beneficial to train on the same experiences multiple times at different stages of training. Even more so since convergence of the Q-values can be slow. The temporal nature of our data will however cause the DQN to be biased to forgetting its early experiences. We don’t simply want the agent to forget what it has seen further in the past.</li>
</ol>
<p>These problems can both have a negative impact on the stability of the training process. Luckily, a solution to both problems is found in experience replay. The idea is simple, we only have to store the agent’s experiences in a memory buffer. This way, we can sample from this buffer during training which both breaks the temporal correlation between data samples, and at the same time allows the model to re-train on previous experiences.</p>
<p>Storing previous experiences and training on them multiple times also has an additional benefit as we can now optimally exploit every single sampled transition we have by controlling how and how often it is “remembered”. This means that we can learn more with the same amount of samples i.e. it is more sample efficient. This is especially beneficial in cases where gaining real-world experience is expensive. Thus, experience replay stabilizes the training process and increases the sample efficiency.</p>
<h2 id="different-types-of-experience-replay-and-environments">Different Types of Experience Replay and Environments</h2>
<p>Now that we have understood why it is important to maintain a memory buffer, we can think of what is the most optimal way to do this. Various experience replay methods have been developed and they mostly differ in two main aspects. Firstly, which experiences do we store in the memory buffer? And secondly, how do we efficiently sample from this memory buffer? How each method handles these questions will influence different types of environments differently and thus each method is typically developed to handle different types of problems. In this blog post we will look at three different ways to employ experience replay.</p>
<h3 id="uniform-experience-replay-er">Uniform Experience Replay (ER)</h3>
<p>Each experience is stored into the buffer and when we reach capacity, we discard our oldest memories. Thus, we keep our most recent memories in the buffer. For sampling, we simply take a random batch of experiences. Each sample has the same probability of being chosen so sample behaviour that is seen more often will therefore also be repeated more and more often.</p>
<h3 id="prioritized-experience-replay-per">Prioritized Experience Replay (PER)</h3>
<p>This method is developed to really exploit samples that display rare or surprising behaviour. The key intuition behind this is that the model can learn more from certain samples than from others, and thus we shouldn’t blindly repeat each of them with equal frequency. Instead, we should prioritize replaying certain samples over others. <em>So how do we determine which samples should be prioritized?</em> Ideally, we would like to know how much the agent can learn from a transition in its current state, but this knowledge is not accessible to us. Luckily, we can approximate this with another metric. Since we are trying to minimize the magnitude of the TD error as an objective function, we can use the absolute TD error <span class="math inline">|<em>δ</em><sub><em>i</em></sub>|</span> as a proxy of how much priority a sample <span class="math inline"><em>i</em></span> should get. Where: <br /><span class="math display"><em>δ</em><sub><em>i</em></sub> = <em>r</em><sub><em>t</em></sub> + <em>λ</em>max<sub><em>a</em> ∈ <em>A</em></sub><em>Q</em><sub><em>θ</em></sub>(<em>s</em><sub><em>t</em> + 1</sub>, <em>a</em>) − <em>Q</em><sub><em>θ</em></sub>(<em>s</em><sub><em>t</em></sub>, <em>a</em><sub><em>t</em></sub>)</span><br /> Now, to store this information during training, we can simply extend the sample transitions we want to store in our memory buffer with this priority proxy: <em>(state, action, reward, new state, <span class="math inline">|<em>δ</em><sub><em>i</em></sub>|</span>)</em>. When the memory buffer reaches its capacity limit, we simply remove the oldest samples. So now we know which samples to store and how to store them, but we still need to find a way to actually use them as intended. This leads us to the second aspect: <em>How do we sample from the memory buffer?</em> We will have to construct a probability distribution where the samples with higher priorities are more likely to be picked for repetition. To get the right priorities for each sample we use the absolute TD error plus some value <span class="math inline"><em>ϵ</em></span> to ensure that each sample in the buffer will be picked with a non-zero probability. We then simply construct a probability distribution as follows: <br /><span class="math display">$$P(i) = \frac{|\delta_{i}|^{\alpha}}{\sum_k |\delta_{i}|^{\alpha}}$$</span><br /> Now to pick the more useful samples with a higher priority, we just have to sample from this distribution!</p>
<h3 id="hindsight-experience-replay-her">Hindsight Experience Replay (HER)</h3>
<p>Introduced by Openai in <a href="http://papers.nips.cc/paper/7090-hindsight-experience-replay.pdf">this paper</a>, this type of experience replay allows our agent to learn from failed experiences. The intuition behind this is that, even when the agent fails, this doesn’t make the experience completely invaluable, the behavior could still be useful in another context. So we don’t just want to dismiss these experiences altogether! HER solves this problem by adapting the sampled transitions that it stores in memory such that it treats failed experiences as successes given the context in which it is used. <em>So how do we store and adapt samples?</em> Since we are now interested in whether experienced episodes were successful or not, we need to store trajectories into our memory buffer with their goal state G: <span class="math inline">(<em>S</em><sub>0</sub>, <em>G</em>, <em>a</em><sub>0</sub>, <em>r</em><sub>0</sub>, <em>S</em><sub>1</sub>), ..., (<em>S</em><sub><em>n</em></sub>, <em>G</em>, <em>a</em><sub><em>n</em></sub>, <em>r</em><sub><em>n</em></sub>, <em>S</em>′)</span> Now the idea in HER is to pretend for failed experiences that the end state S’ was actually the goal G all along. To do this, we just substitute G with S’ and store this imagined trajectory into memory as well: <span class="math inline">(<em>S</em><sub>0</sub>, <em>S</em>′, <em>a</em><sub>0</sub>, <em>r</em><sub>0</sub>, <em>S</em><sub>1</sub>), ..., (<em>S</em><sub><em>n</em></sub>, <em>S</em>′, <em>a</em><sub><em>n</em></sub>, <em>r</em><sub><em>n</em></sub>, <em>S</em>′)</span> In this alternative reality the agent has reached the goal and got a positive reward for doing so. By storing both the real and the imagined trajectory into memory we can ensure that the agent always gets some positive reward to learn from. As the agent can imagine reaching multiple goals at the end of an episode, the agent can maximally learn from this episode.<br />
As a result, HER can also be effectively used in multi-goal settings. <em>So how do we sample from this memory buffer?</em> This is simple, we just take a random batch of experiences like in ER.</p>
<h3 id="environments">Environments</h3>
<p>As mentioned earlier, these different forms of experience replay will have a different impact on different types of environments. In this blog post we will focus on three types of deterministic environments with different characteristics:</p>
<ul>
<li><p><a href="https://github.com/podondra/gym-gridworlds">CliffGridworld-v0</a> In the cliffworld environment the agent has to move from the starting state (S) to the goal state (G), it is a classic RL example. We would like to test the performance of the different experience replays, across multiple difficulty levels. We chose this environment as an example of a relatively simple environment, although interesting, compared to the others. It has a two-dimensional discrete state space. <img src="plots/cliffworld.png" alt="Gridworld environment" /></p></li>
<li><p><a href="https://gym.openai.com/envs/Acrobot-v1/">Acrobot-v1</a> Acrobot steps up the difficulty from the cliffworld. The agent has to swing any part of the arm above the line. Here we clearly see that it is a more challenging environment, evident by the fact that it has a six dimensional continuous state space.</p></li>
<li><p><a href="https://gym.openai.com/envs/CartPole-v1/">CartPole-v1</a></p>
<p>This environment requires the agent to balance a pole on a cart, hence the name. It has to do so for as long as possible. It has a 4-dimensional continuous input, of which 2 are of infinite magnitude, and 2 discrete actions, push left and push right. <img src="gifs/cartpole_prio.gif" alt="cartpole" title="Cartpole environment" width="300"
 align="middle" /></p></li>
<li><p><a href="https://gym.openai.com/envs/MountainCar-v0/">MountainCar-v0</a></p>
<p>In this environment the agent needs to get the cart to the top of the mountain as fast as possible. It does not however have enough momentum to just drive up the mountain. It needs to drive left and right a few times to gain momentum. This environment is tricky because the agent only gets a reward for reaching the top and not while trying to gain momentum.</p>
<p><img src="gifs/mountaincar_her.gif" alt="Mountaincar" title="A trained agent in the mountaincar environment." width="300"
 align="middle" /></p></li>
</ul>
<h2 id="what-will-we-investigate">What will we investigate?</h2>
<p>We will investigate the behaviour of the introduced experience replay methods on the environments we just proposed. This is interesting as the method of experience replay and the type of task it is used on, may heavily influence the performance of the model. For this we form the following hypothesis: we expect PER to perform better on the more complex environment and HER to perform better on the mountain car environment, which has a sparse reward. For the simple environment we think that ER will be sufficient and that using PER or HER might not provide a competitive advantage.</p>
<p>Also, as you recall, we explained earlier that experience replay should have a positive influence on the training stability and the sample efficiency. Thus, we will compare the influence of each method on each environment w.r.t. the number of training steps, samples needed for convergence, and the cumulative reward.</p>
<p>Apart from this, according to this <a href="https://arxiv.org/pdf/1712.01275.pdf">paper by Zhang &amp; Sutton</a>, there is another important component to experience replay which effect has been underestimated: the memory buffer size. They show that for different environments, different buffer sizes are optimal. For example, uniform experience replay does not stimulate the algorithm much to use recent transitions, except by throwing out the oldest experiences when the maximum capacity is reached. However, when the buffer size is set to e.g. <span class="math inline">10<sup>6</sup></span> then the probability of using recent transitions, once the buffer is reaching maximum capacity, is very small. If too little recent experiences are used, this can negatively effect the performance in which case we would say that the buffer size is too large. Contrary, you could not use enough older experiences i.e. set your buffer size too small as well.</p>
<p>Whether buffer sizes are too large or too small depends on your environment. Thus, we perform an additional experiment, where we will run each form of experience replay on the gridworld environment with a range of different buffer sizes. From these we will use the results from the most optimal buffer size for investigation.</p>
<p>Since the randomness in the architecture can affect the results, we run the model 5 times with different random seeds and report the average and variance over these results.</p>
<h2 id="implementation-details">Implementation Details</h2>
<h3 id="deep-q-network">Deep Q-Network</h3>
<p>We use a simple DQN that is trained with the Adam optimizer.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="kw">class</span> QNetwork(nn.Module):</span>
<span id="cb1-2"><a href="#cb1-2"></a>    </span>
<span id="cb1-3"><a href="#cb1-3"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, input_dim, num_hidden<span class="op">=</span><span class="dv">512</span>, output_dim):</span>
<span id="cb1-4"><a href="#cb1-4"></a>        nn.Module.<span class="fu">__init__</span>(<span class="va">self</span>)</span>
<span id="cb1-5"><a href="#cb1-5"></a>        <span class="va">self</span>.l1 <span class="op">=</span> nn.Linear(input_dim, num_hidden)</span>
<span id="cb1-6"><a href="#cb1-6"></a>        <span class="va">self</span>.l2 <span class="op">=</span> nn.Linear(num_hidden, output_dim)</span>
<span id="cb1-7"><a href="#cb1-7"></a></span>
<span id="cb1-8"><a href="#cb1-8"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb1-9"><a href="#cb1-9"></a>        relu_output <span class="op">=</span> F.relu(<span class="va">self</span>.l1(x))</span>
<span id="cb1-10"><a href="#cb1-10"></a>        out <span class="op">=</span> <span class="va">self</span>.l2(relu_output)</span>
<span id="cb1-11"><a href="#cb1-11"></a>        <span class="cf">return</span> out</span></code></pre></div>
<p>For the learning rate <span class="math inline"><em>α</em></span> and discount factor <span class="math inline"><em>γ</em></span> we first perform a grid search over <span class="math inline"><em>α</em> = [0.0001, 0.0005, 0.001]</span> and <span class="math inline"><em>γ</em> = [0.7, 0.75, 0.8, 0.99]</span> for each environment. Since the tasks we train on are very different, we can not just use the hyperparameter values that perform well on one environment and expect it to generalize well to the others. For the first three games it is sufficient to train the agent for 300 episodes, but through experimentation we found that MountainCar needs 1000 episodes to converge.</p>
<table>
<thead>
<tr class="header">
<th></th>
<th><span class="math inline"><em>α</em></span></th>
<th><span class="math inline"><em>γ</em></span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Cliffworld</td>
<td>0.0001</td>
<td>0.8</td>
</tr>
<tr class="even">
<td>Acrobot</td>
<td>0.001</td>
<td>0.99</td>
</tr>
<tr class="odd">
<td>Cartpole</td>
<td>0.001</td>
<td>0.8</td>
</tr>
<tr class="even">
<td>Mountain Car</td>
<td>0.001</td>
<td>0.9</td>
</tr>
</tbody>
</table>
<p>Thus, we use the same model with different hyperparameter values for each environment, but the model remains constant for each of the ER methods. Since we are interested in the effect of the ER methods in each environment, this is a fair comparison.</p>
<h3 id="prioritized-experience-replay">Prioritized Experience Replay</h3>
<p>The implementation of PER is based on the code from <a href="https://github.com/rlcode/per/blob/master/prioritized_memory.py">this</a> GitHub. The hyperparameter <span class="math inline"><em>α</em></span> controls the level of prioritization that is applied, when <span class="math inline"><em>α</em> → 0</span> there is no prioritization, whereas, when <span class="math inline"><em>α</em> → 1</span> there is full prioritization. We don’t want to apply full prioritization as it would cause our model to overfit. Therefore, we assign <span class="math inline"><em>α</em></span> a value of 0.6 which was found in the <a href="https://arxiv.org/pdf/1511.05952.pdf">original PER paper</a> by using a coarse grid-search.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1"></a><span class="kw">class</span> PrioritizedER():</span>
<span id="cb2-2"><a href="#cb2-2"></a></span>
<span id="cb2-3"><a href="#cb2-3"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, capacity, n_episodes, alpha<span class="op">=</span><span class="fl">0.6</span>, beta<span class="op">=</span><span class="fl">0.4</span>):</span>
<span id="cb2-4"><a href="#cb2-4"></a>        <span class="va">self</span>.alpha <span class="op">=</span> alpha</span>
<span id="cb2-5"><a href="#cb2-5"></a>        <span class="va">self</span>.beta <span class="op">=</span> beta</span>
<span id="cb2-6"><a href="#cb2-6"></a>        <span class="va">self</span>.capacity <span class="op">=</span> capacity</span>
<span id="cb2-7"><a href="#cb2-7"></a>        <span class="va">self</span>.beta_increment_per_sampling <span class="op">=</span> <span class="fl">0.001</span></span>
<span id="cb2-8"><a href="#cb2-8"></a>        <span class="va">self</span>.e <span class="op">=</span> <span class="fl">10e-2</span></span>
<span id="cb2-9"><a href="#cb2-9"></a>        <span class="va">self</span>.tree <span class="op">=</span> SumTree(capacity)</span>
<span id="cb2-10"><a href="#cb2-10"></a></span>
<span id="cb2-11"><a href="#cb2-11"></a>    <span class="kw">def</span> push(<span class="va">self</span>, transition, error):</span>
<span id="cb2-12"><a href="#cb2-12"></a>        delta <span class="op">=</span> (<span class="bu">abs</span>(error) <span class="op">+</span> <span class="va">self</span>.e) <span class="op">**</span> <span class="va">self</span>.alpha</span>
<span id="cb2-13"><a href="#cb2-13"></a>        <span class="va">self</span>.tree.add(delta, transition)</span>
<span id="cb2-14"><a href="#cb2-14"></a></span>
<span id="cb2-15"><a href="#cb2-15"></a>    <span class="kw">def</span> sample(<span class="va">self</span>, batch_size):</span>
<span id="cb2-16"><a href="#cb2-16"></a>        batch <span class="op">=</span> []</span>
<span id="cb2-17"><a href="#cb2-17"></a>        idxs <span class="op">=</span> []</span>
<span id="cb2-18"><a href="#cb2-18"></a>        segment <span class="op">=</span> <span class="va">self</span>.tree.total() <span class="op">/</span> batch_size</span>
<span id="cb2-19"><a href="#cb2-19"></a>        priorities <span class="op">=</span> []</span>
<span id="cb2-20"><a href="#cb2-20"></a>        <span class="va">self</span>.beta <span class="op">=</span> np.<span class="bu">min</span>([<span class="fl">1.</span>, <span class="va">self</span>.beta <span class="op">+</span> <span class="va">self</span>.beta_increment_per_sampling])</span>
<span id="cb2-21"><a href="#cb2-21"></a></span>
<span id="cb2-22"><a href="#cb2-22"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(batch_size):</span>
<span id="cb2-23"><a href="#cb2-23"></a>            a <span class="op">=</span> segment <span class="op">*</span> i</span>
<span id="cb2-24"><a href="#cb2-24"></a>            b <span class="op">=</span> segment <span class="op">*</span> (i <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb2-25"><a href="#cb2-25"></a></span>
<span id="cb2-26"><a href="#cb2-26"></a>            s <span class="op">=</span> random.uniform(a, b)</span>
<span id="cb2-27"><a href="#cb2-27"></a>            (idx, p, data) <span class="op">=</span> <span class="va">self</span>.tree.get(s)</span>
<span id="cb2-28"><a href="#cb2-28"></a>            priorities.append(p)</span>
<span id="cb2-29"><a href="#cb2-29"></a>            batch.append(data)</span>
<span id="cb2-30"><a href="#cb2-30"></a>            idxs.append(idx)</span>
<span id="cb2-31"><a href="#cb2-31"></a></span>
<span id="cb2-32"><a href="#cb2-32"></a>        sampling_probabilities <span class="op">=</span> priorities <span class="op">/</span> <span class="va">self</span>.tree.total() <span class="op">+</span> <span class="fl">10e-5</span></span>
<span id="cb2-33"><a href="#cb2-33"></a>        is_weight <span class="op">=</span> np.power(<span class="va">self</span>.tree.n_entries <span class="op">*</span> sampling_probabilities, <span class="op">-</span><span class="va">self</span>.beta)</span>
<span id="cb2-34"><a href="#cb2-34"></a>        is_weight <span class="op">/=</span> is_weight.<span class="bu">max</span>()</span>
<span id="cb2-35"><a href="#cb2-35"></a>        <span class="cf">return</span> batch, idxs, is_weight</span></code></pre></div>
<p>Furthermore, it would be costly to store the transitions in a list, as we would have to traverse the whole list and compare all the <span class="math inline">|<em>δ</em><sub><em>i</em></sub>|</span> values. As a solution, the paper proposes a sum-tree data structure to store the transitions, as a result we now achieve a complexity of <span class="math inline"><em>O</em>log <em>N</em></span> when updating and sampling. We used <a href="https://github.com/rlcode/per/blob/master/SumTree.py">this</a> code to implement the sum-tree.</p>
<h3 id="hindsight-experience-replay">Hindsight Experience Replay</h3>
<p>The implementation of Hindsight Experience Replay is based on <a href="https://github.com/orrivlin/Hindsight-Experience-Replay---Bit-Flipping">this</a> and <a href="https://github.com/openai/baselines/tree/master/baselines/her">this</a> implementation.</p>
<p>Since we are dealing with environments that have only one goal, our implementation is quite simple, as we do not have to change the goal in any non-terminal states. Instead, we only change the achieved value in the end state of an episode. One parameter called ‘replay <span class="math inline"><em>k</em></span>’ is introduced which sets the ratio of HER replays versus normal replays in the buffer. We set ‘replay k’ to 4 as that is what is also used by OpenAI in their experiments. Concretely this means that every episode is inserted into the buffer normally, and additionally the same episode is inserted <span class="math inline"><em>k</em></span> times (the hindsight replays), with an altered goal with reward 0.</p>
<h2 id="results">Results</h2>
<p>First we’ll look at the behaviour of each type of experience replay in every environment separately. The results are shown for the buffer size that performed best on each environment. Furthermore, if you recall, using experience replay could positively affect the stability of the training process and the sample efficiency. Thus, we will analyze both the variance over the performance and the time it takes for each method to converge. <img src="./plots/replay_types.png" title="Plot of replay types on the environments" alt="plot1" /></p>
<p>In the cliff environment we can see that, in agreement with our hypothesis, the ER method is sufficient to obtain reasonable performance and HER and PER do not provide any competitive advantage. The behaviour of PER is, however, surprising as its performance considerably worse than ER and HER.</p>
<p>In cartpole we observe high variance for all replay types and it’s hard to say which method should be preferred, however, PER seems to learn the fastest albeit with the highest variance. Since after 250 episodes the performance of all converge to approximately the same results, which method should be preferred depends on the objective that we are trying to optimize for. If we want a more stable training process, ER seems to be best, but if we want the most sample efficient method we should choose PER as it converges the fastest.</p>
<p>With acrobot, all replay types converge to similar performances, but PER converges the fastest. Since all methods also have a similar variance, this time around PER is probably the best bet.</p>
<p>Mountain car again exhibits roughly the same performance for each replay type. It seems as though PER can perform slightly better at the cost of high variance.</p>
<p>Furthermore, we could not see a negative effect of the buffer size when only considering buffer sizes of 3000, 10.000 and 30.000. Therefore, we performed an additional experiment where we tested all the sizes described in the paper. Due to time constraints, we only tested this on the GirdWorld environment and the results are shown below.</p>
<figure>
<img src="./plots/buffer_zoomed.png" title="Plot of buffer size impact on the cliff environment" alt="" /><figcaption>plot2</figcaption>
</figure>
<p>We can see that for each type of experience replay a buffer size of 1000 performs the best and the higher the buffer size gets, the worse it performs. A buffer size of 100 being the exception, this performed so badly it does not show up in our figures.</p>
<p>The reason for this is that with a higher buffer size, older transitions are used, transitions which were sampled from an older and different policy. This, in combination with the fact that Deep Q Networks make no use of importance sampling, could introduce a bias in the network, which would explain the decrease in performance. The abysmal performance of buffer size 100 is due to the fact that the network is overfitting on the most recent transitions.</p>
<h2 id="conclusion">Conclusion</h2>
<p>In conclusion, the environments we chose, do not seem to be very sensitive to these different types of experience replays. On average ER seems to be sufficient to solve these problems, and implementing more sophisticated methods do not seem worth it. This is not completely unsurprising as PER and HER were specifically developed with more complex and different problems in mind. For example, HER did not perform well on our chosen tasks, but its benefits in the original paper are mostly shown on multi-goal setting which we did not test in this blog post.</p>
<p>Furthermore, we saw that a too small or too big buffer size has indeed a negative impact on the performance. Even so much so that in these environments the buffer size seemed to matter more than the type of ER method that is used. Thus, when using DQN’s it is important to also spend some time on optimizing the buffer size you use! Additionally, we observed that during different stages of training different buffer sizes seemed optimal, it would be interesting to see whether dynamically changing the buffer size could be beneficial. In addition, the <a href="https://arxiv.org/pdf/1712.01275.pdf">paper by Zhang &amp; Sutton</a> also proposes a solution to diminish the negative impact of a suboptimally chosen buffer size. This solution is called combined experience replay (CER), an extension to uniform experience replay. Unfortunately, applying this method was outside the scope of this project. This would also be interesting for further research.</p>
